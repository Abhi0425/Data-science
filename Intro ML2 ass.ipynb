{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586972be-6ac2-47de-b886-edcd83d38e42",
   "metadata": {},
   "source": [
    "ans 1\n",
    "overfitting-Overfitting is an undesirable machine learning behavior that occurs when the machine learning model gives accurate predictions for training data but not for new data.\n",
    "\n",
    "Consequnces of overfitting\n",
    "\n",
    "When the model memorizes the noise and fits too closely to the training set, the model becomes “overfitted,” and it is unable to generalize well to new data. If a model cannot generalize well to new data, then it will not be able to perform the classification or prediction tasks that it was intended for.\n",
    "how to mitigate?\n",
    " the methods used to prevent overfitting include ensembling, data augmentation, data simplification, and cross-validation.\n",
    "\n",
    "Underfitting- Underfitting describes a model which does not capture the underlying relationship in the dataset on which it's trained.\n",
    "\n",
    "consequences of underfitting\n",
    "\n",
    "When a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks.\n",
    "\n",
    "how to mitigate?\n",
    "Increase the number of features in the dataset.\n",
    "Increase model complexity.\n",
    "Reduce noise in the data.\n",
    "Increase the duration of training the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaefe1d-8b09-4c63-92b7-b421ebe5cb40",
   "metadata": {},
   "source": [
    "Ans2 \n",
    "we can reduce overfitting by some of the methods\n",
    "ensembling, data augmentation, data simplification, and cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4fd035-7e84-4945-80cd-e8618d097337",
   "metadata": {},
   "source": [
    "Ans 3\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output\n",
    "variables accurately, generating a high error rate on both thetraining set and unseen data. It occurs when a model is too simple, \n",
    "which can be a result of a model needing more training time, more input features, or less regularization. Like overfitting,\n",
    "when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance \n",
    "of the model. If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks. \n",
    "Generalization of a model to new data is ultimately what allows us to use machine learning algorithms every day to make predictions and\n",
    "classify data.\n",
    "\n",
    "High bias and low variance are good indicators of underfitting. Since this behavior can be seen while using the training dataset, \n",
    "underfitted models are usually easier to identify than overfitted ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c6d83-2dc5-406b-9bc3-da7a8085f8cf",
   "metadata": {},
   "source": [
    "Ans4\n",
    "the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\n",
    "\n",
    "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance\n",
    "\n",
    "A model with a high bias error underfits data and makes very simplistic assumptions on it. A model with a high variance error overfits the data and learns too much from it. A good model is where both Bias and Variance errors are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4eb04b-9ab5-4183-b5ba-bf054b049c68",
   "metadata": {},
   "source": [
    "Ans 5\n",
    "Train , test and validation are the common methods for detecting overfitting anf underfittting\n",
    "\n",
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. Your model is underfitting the training data when the model performs poorly on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783845a-7267-4f5c-bd0f-ce71b3b8938d",
   "metadata": {},
   "source": [
    "Ans 6\n",
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.\n",
    "\n",
    " Linear Regression, Linear Discriminant Analysis and Logistic Regression are the examples of high bias\n",
    "Decision Trees, k-Nearest Neighbors and Support Vector Machine are the example of hogh variance\n",
    "\n",
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18705d3c-d981-4293-893d-4cf57b113c53",
   "metadata": {},
   "source": [
    "Ans 7\n",
    "Regularization is a technique that adds information to a model to prevent the occurrence of overfitting\n",
    "\n",
    "regularizing technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "\n",
    "L1 and L2 are the most common types of regularization.\n",
    "\n",
    "L1 Regularization, also called a lasso regression, adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function. L2 Regularization, also called a ridge regression, adds the “squared magnitude” of the coefficient as the penalty term to the loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
